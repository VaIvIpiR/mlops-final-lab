import os
import sagemaker
from sagemaker.workflow.pipeline import Pipeline
from sagemaker.workflow.steps import TrainingStep, CreateModelStep, ProcessingStep
from sagemaker.workflow.step_collections import RegisterModel
from sagemaker.workflow.parameters import ParameterInteger, ParameterString
from sagemaker.pytorch import PyTorch, PyTorchModel
from sagemaker.inputs import TrainingInput
from sagemaker.sklearn.processing import SKLearnProcessor
from sagemaker.processing import ProcessingInput, ProcessingOutput

# --- –í–ê–®–Ü –ù–û–í–Ü –†–ï–°–£–†–°–ò –ó TERRAFORM ---
TERRAFORM_BUCKET = "mlops-lab-terraform-vaivipir-data"
TERRAFORM_ROLE = "arn:aws:iam::584360834542:role/mlops-lab-terraform-role"

def get_pipeline(
    region,
    role=None,
    default_bucket=None,
    pipeline_name="NewsClassifierPipeline",
    model_package_group_name="NewsClassifierGroup",
    base_job_prefix="NewsClassifier"
):
    """
    –§—É–Ω–∫—Ü—ñ—è, —è–∫–∞ –ø–æ–≤–µ—Ä—Ç–∞—î –æ–±'—î–∫—Ç Pipeline.
    """
    sess = sagemaker.Session()
    
    if default_bucket is None:
        default_bucket = TERRAFORM_BUCKET

    # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏
    training_instance_type = ParameterString(name="TrainingInstanceType", default_value="ml.m5.large")
    
    input_data = ParameterString(
        name="InputData", 
        default_value=f"s3://{default_bucket}/data/raw/"
    )
    
    epochs = ParameterInteger(name="Epochs", default_value=3)
    batch_size = ParameterInteger(name="BatchSize", default_value=8)

    # --- –ö—Ä–æ–∫ 1: –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è ---
    estimator = PyTorch(
        entry_point='train_sagemaker.py',
        source_dir='./src', 
        role=role,
        framework_version='1.13',
        py_version='py39',
        instance_count=1,
        instance_type=training_instance_type,
        output_path=f"s3://{default_bucket}/NewsClassifier/training_jobs",
        hyperparameters={'epochs': epochs, 'batch-size': batch_size, 'learning-rate': 2e-5},
        environment={
            'MLFLOW_TRACKING_URI': 'https://dagshub.com/vaivipir/news-classifier.mlflow',
            'MLFLOW_TRACKING_USERNAME': 'vaivipir',
            'MLFLOW_TRACKING_PASSWORD': 'a38c18a9ac6a76a6d22f38feae7cc1e984dff094' 
        }
    )

    step_train = TrainingStep(
        name="BertTrainingStep",
        estimator=estimator,
        inputs={"training": TrainingInput(s3_data=input_data)}
    )

    # --- –ö—Ä–æ–∫ 2: –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ ---
    model = PyTorchModel(
        model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,
        role=role,
        entry_point='inference.py',
        source_dir='./src',
        framework_version='1.13',
        py_version='py39',
        sagemaker_session=sess
    )

    step_create_model = CreateModelStep(
        name="BertCreateModelStep",
        model=model,
        inputs=sagemaker.inputs.CreateModelInput(instance_type="ml.m5.large")
    )

    # --- –ö—Ä–æ–∫ 2.5: –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –î—Ä–∏—Ñ—Ç—É (Drift Check) ---
    sklearn_processor = SKLearnProcessor(
        framework_version="1.2-1",
        role=role,
        instance_type="ml.t3.medium",
        instance_count=1,
        base_job_name="drift-check"
    )

    step_drift = ProcessingStep(
        name="DriftCheckStep",
        processor=sklearn_processor,
        code="./src/check_drift.py",
        inputs=[
            ProcessingInput(
                source=input_data, 
                destination="/opt/ml/processing/input" 
            )
        ],
        outputs=[
            ProcessingOutput(output_name="report", source="/opt/ml/processing/output")
        ]
    )
    
    # --- –ö—Ä–æ–∫ 3: –†–µ—î—Å—Ç—Ä–∞—Ü—ñ—è ---
    step_register = RegisterModel(
        name="BertRegisterStep",
        estimator=estimator,
        model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,
        content_types=["application/json"],
        response_types=["application/json"],
        inference_instances=["ml.m5.large"],
        transform_instances=["ml.m5.large"],
        model_package_group_name=model_package_group_name,
        approval_status="PendingManualApproval"
    )
    
    # –ú–ò –í–ò–î–ê–õ–ò–õ–ò add_depends_on, –±–æ RegisterModel —Ü–µ –Ω–µ –ø—ñ–¥—Ç—Ä–∏–º—É—î.
    # –í–æ–Ω–∏ –ø—Ä–æ—Å—Ç–æ –∑–∞–ø—É—Å—Ç—è—Ç—å—Å—è –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ –ø—ñ—Å–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è/—Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ.

    # --- –ó–ë–ò–†–ê–Ñ–ú–û –ü–ê–ô–ü–õ–ê–ô–ù ---
    pipeline = Pipeline(
        name=pipeline_name,
        parameters=[training_instance_type, input_data, epochs, batch_size],
        # –í–ê–ñ–õ–ò–í–û: –ü–µ—Ä–µ–¥–∞—î–º–æ –í–°–Ü –∫—Ä–æ–∫–∏ —Å—é–¥–∏
        steps=[step_train, step_create_model, step_drift, step_register] 
    )
    
    return pipeline

if __name__ == "__main__":
    print("üöÄ Building pipeline using Terraform resources...")
    
    import sys
    
    role_arn = os.environ.get("SAGEMAKER_ROLE_ARN", TERRAFORM_ROLE)
    
    print(f"üîë Using Role: {role_arn}")
    print(f"ü™£ Using Bucket: {TERRAFORM_BUCKET}")

    pipeline = get_pipeline(
        region=os.environ.get("AWS_REGION", "eu-north-1"),
        role=role_arn,
        default_bucket=TERRAFORM_BUCKET
    )
    
    print(f"üìù Pipeline definition: {pipeline.name}")
    
    pipeline.upsert(role_arn=role_arn)
    print("‚úÖ Pipeline submitted/updated in SageMaker.")
    
    execution = pipeline.start()
    print(f"üèÉ Pipeline execution started. Execution ARN: {execution.arn}")